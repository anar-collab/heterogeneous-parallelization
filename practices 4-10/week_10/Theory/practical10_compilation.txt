===========================================
ИНСТРУКЦИИ ПО КОМПИЛЯЦИИ И ЗАПУСКУ
Практическая работа №10
===========================================

ЗАДАНИЕ 1: Анализ производительности OpenMP
-----------------------------------------

Компиляция:
g++ -fopenmp practical10_task1_openmp.cpp -o task1_openmp

Запуск:
./task1_openmp

Что программа делает:
- Вычисляет сумму, среднее и дисперсию для массива 100 млн элементов
- Сравнивает последовательную и параллельную версии
- Тестирует с 1, 2, 4, 8, 16 потоками
- Анализирует результаты по закону Амдала
- Выводит ускорение и эффективность для каждого случая

Примечания:
- Требуется компилятор с поддержкой OpenMP (gcc 4.2+, clang 3.7+)
- Можно изменить размер массива в коде (arraySize)
- Время выполнения зависит от процессора


ЗАДАНИЕ 2: Оптимизация доступа к памяти CUDA
-----------------------------------------

Компиляция:
nvcc practical10_task2_cuda_memory.cu -o task2_cuda_memory

Запуск:
./task2_cuda_memory

Что программа делает:
- Тестирует 4 паттерна доступа к памяти GPU:
  1. Коалесцированный доступ (базовая линия)
  2. Доступ с шагом (неэффективный)
  3. С использованием разделяемой памяти
  4. Оптимизированная организация потоков
- Измеряет время каждого варианта с cudaEvent
- Сравнивает производительность
- Анализирует влияние паттернов доступа

Примечания:
- Требуется CUDA Toolkit и NVIDIA GPU
- Проверить GPU: nvidia-smi
- Размер массива: 10 млн элементов


ЗАДАНИЕ 3: Профилирование гибридного приложения
-----------------------------------------

Компиляция:
nvcc practical10_task3_hybrid.cu -o task3_hybrid -lpthread

Запуск:
./task3_hybrid

Что программа делает:
- Сравнивает 4 подхода:
  1. Только CPU
  2. Только GPU (синхронная передача)
  3. Гибридный базовый (CPU + GPU параллельно)
  4. Оптимизированный гибридный (async + streams)
- Измеряет накладные расходы передачи данных
- Анализирует узкие места CPU-GPU взаимодействия
- Демонстрирует эффект от оптимизаций

Примечания:
- Требуется CUDA и поддержка потоков C++11
- Использует pinned memory и CUDA streams
- Обрабатывает 10 млн элементов
- Флаг -lpthread для поддержки std::thread


ЗАДАНИЕ 4: Анализ масштабируемости MPI
-----------------------------------------

Компиляция:
mpic++ practical10_task4_mpi_scaling.cpp -o task4_mpi_scaling

Запуск с разным количеством процессов:

2 процесса:
mpirun -np 2 ./task4_mpi_scaling

4 процесса:
mpirun -np 4 ./task4_mpi_scaling

8 процессов:
mpirun -np 8 ./task4_mpi_scaling

Что программа делает:
- Тестирует Strong Scaling (фиксированный размер задачи)
- Тестирует Weak Scaling (размер растёт с процессами)
- Сравнивает MPI_Reduce и MPI_Allreduce
- Анализирует балансировку нагрузки
- Оценивает масштабируемость по законам Амдала и Густафсона

Примечания:
- Требуется OpenMPI или MPICH
- Strong scaling: 100 млн элементов
- Weak scaling: 10 млн элементов на процесс
- Можно запускать на одной машине или кластере


ОБЩИЕ ТРЕБОВАНИЯ
-----------------------------------------

Для всех заданий:
- C++ компилятор с поддержкой C++11
- Достаточно оперативной памяти (минимум 2 ГБ)

Для заданий 1:
- GCC 4.2+ или Clang 3.7+ с OpenMP
- Установка OpenMP: обычно встроена в GCC

Для заданий 2-3:
- CUDA Toolkit 10.0+
- NVIDIA GPU с compute capability 3.0+
- Драйверы NVIDIA

Для задания 4:
- OpenMPI или MPICH
- Установка на Ubuntu: sudo apt install openmpi-bin libopenmpi-dev


ПРОВЕРКА УСТАНОВКИ КОМПОНЕНТОВ
-----------------------------------------

Проверка OpenMP:
echo |cpp -fopenmp -dM |grep -i open

Проверка CUDA:
nvcc --version
nvidia-smi

Проверка MPI:
mpirun --version


КОМПИЛЯЦИЯ ВСЕХ ПРОГРАММ СРАЗУ
-----------------------------------------

# OpenMP
g++ -fopenmp practical10_task1_openmp.cpp -o task1_openmp

# CUDA задание 2
nvcc practical10_task2_cuda_memory.cu -o task2_cuda_memory

# CUDA задание 3
nvcc practical10_task3_hybrid.cu -o task3_hybrid -lpthread

# MPI
mpic++ practical10_task4_mpi_scaling.cpp -o task4_mpi_scaling


ЗАПУСК ПОЛНОГО ТЕСТИРОВАНИЯ
-----------------------------------------

# Задание 1
echo "=== Задание 1: OpenMP ==="
./task1_openmp

# Задание 2
echo "=== Задание 2: CUDA Memory ==="
./task2_cuda_memory

# Задание 3
echo "=== Задание 3: Гибридное ==="
./task3_hybrid

# Задание 4
echo "=== Задание 4: MPI 2 процесса ==="
mpirun -np 2 ./task4_mpi_scaling

echo "=== Задание 4: MPI 4 процесса ==="
mpirun -np 4 ./task4_mpi_scaling

echo "=== Задание 4: MPI 8 процессов ==="
mpirun -np 8 ./task4_mpi_scaling


ПРОФИЛИРОВАНИЕ С ВНЕШНИМИ ИНСТРУМЕНТАМИ
-----------------------------------------

gprof (для CPU):
g++ -fopenmp -pg practical10_task1_openmp.cpp -o task1_openmp
./task1_openmp
gprof task1_openmp gmon.out > analysis.txt

Intel VTune (если установлен):
vtune -collect hotspots ./task1_openmp

NVIDIA Nsight Compute (для CUDA):
ncu --set full -o profile_report ./task2_cuda_memory

NVIDIA Nsight Systems:
nsys profile -o timeline ./task3_hybrid


ОЖИДАЕМЫЕ РЕЗУЛЬТАТЫ
-----------------------------------------

Задание 1 (OpenMP):
- Последовательная версия: ~1-2 секунды
- Параллельная 4 потока: ускорение 3-3.5x
- Параллельная 8 потоков: ускорение 5-6x
- Эффективность падает с ростом потоков

Задание 2 (CUDA Memory):
- Коалесцированный: базовая линия
- Stride: медленнее в 2-5x
- Shared memory: быстрее на 10-30%
- Оптимизированный: быстрее на 20-50%

Задание 3 (Гибридное):
- CPU only: базовая линия
- GPU only: может быть медленнее из-за передачи
- Гибридный базовый: лучше CPU
- Оптимизированный: лучший результат

Задание 4 (MPI):
- 2 процесса: ускорение ~1.8x
- 4 процесса: ускорение ~3-3.5x
- 8 процессов: ускорение ~5-6x
- Weak scaling: время растёт медленно


ЧАСТЫЕ ПРОБЛЕМЫ И РЕШЕНИЯ
-----------------------------------------

Проблема: "fopenmp: No such file"
Решение: Установить gcc с OpenMP или использовать флаг -fopenmp=libomp

Проблема: "nvcc: command not found"
Решение: Установить CUDA Toolkit и добавить в PATH

Проблема: "no CUDA-capable device"
Решение: Проверить GPU через nvidia-smi, установить драйверы

Проблема: "mpirun: command not found"
Решение: Установить OpenMPI (sudo apt install openmpi-bin)

Проблема: Программа падает с segmentation fault
Решение: Проверить достаточно ли памяти, уменьшить arraySize

Проблема: GPU программы очень медленные
Решение: Проверить что используется GPU а не эмуляция, nvidia-smi

Проблема: MPI не находит процессы
Решение: Проверить /etc/hosts, использовать --hostfile для кластера


НАСТРОЙКА РАЗМЕРОВ ДЛЯ БЫСТРОГО ТЕСТИРОВАНИЯ
-----------------------------------------

Если программы выполняются слишком долго, можно уменьшить размеры в коде:

Задание 1:
const size_t arraySize = 10000000;  // было 100000000

Задание 2:
const int arraySize = 1000000;  // было 10000000

Задание 3:
const int arraySize = 1000000;  // было 10000000

Задание 4:
const long long strongScalingSize = 10000000;  // было 100000000
const long long sizePerProcess = 1000000;  // было 10000000


ЗАПУСК НА УДАЛЁННОМ КЛАСТЕРЕ
-----------------------------------------

Для MPI на кластере создайте hostfile:
echo "node1 slots=4" > hostfile
echo "node2 slots=4" >> hostfile

Запуск:
mpirun --hostfile hostfile -np 8 ./task4_mpi_scaling


ДОПОЛНИТЕЛЬНЫЕ ЭКСПЕРИМЕНТЫ
-----------------------------------------

1. Изменить размер массивов и посмотреть как меняется производительность
2. Попробовать разное количество потоков/процессов
3. Изменить соотношение CPU/GPU в гибридном приложении
4. Сравнить MPI_Reduce с MPI_Bcast + локальные вычисления
5. Добавить больше CUDA streams в задании 3


ПОЛЕЗНЫЕ КОМАНДЫ
-----------------------------------------

Узнать количество ядер CPU:
lscpu | grep "^CPU(s)"

Узнать информацию о GPU:
nvidia-smi -L
nvidia-smi --query-gpu=name,memory.total --format=csv

Мониторинг GPU в реальном времени:
watch -n 1 nvidia-smi

Проверить загрузку CPU:
htop  # или top
